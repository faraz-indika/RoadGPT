{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK\n",
    "! pip install --user --upgrade google-cloud-aiplatform==1.47.0 langchain==0.1.14 langchain-google-vertexai==0.1.3 typing_extensions==4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tabulate import tabulate\n",
    "import pdfplumber\n",
    "from operator import itemgetter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import vertexai\n",
    "from langchain_google_vertexai import (\n",
    "    VertexAI,\n",
    "    VertexAIEmbeddings,\n",
    "    VectorSearchVectorStore,\n",
    ")\n",
    "from google.cloud import aiplatform\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_PATH = './PDFs/'\n",
    "PROJECT_ID = \"<project_id>\"  # @param {type:\"string\"}\n",
    "REGION = \"<region>\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf_loader:\n",
    "\n",
    "    def __init__(self, directory_path) -> None:\n",
    "        self.directory_path = directory_path\n",
    "        self.pdfs = [directory_path + pdf for pdf in os.listdir(directory_path)]\n",
    "\n",
    "    def check_bboxes(self, word, table_bbox):\n",
    "        l = word['x0'], word['top'], word['x1'], word['bottom']\n",
    "        r = table_bbox\n",
    "        return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]\n",
    "\n",
    "    def format_table(self, table):\n",
    "        label = table[0][0]\n",
    "        for lb_ind in range(len(table[0])):\n",
    "            if table[0][lb_ind]:\n",
    "                label = table[0][lb_ind]\n",
    "            else:\n",
    "                table[0][lb_ind] = label\n",
    "        return str(tabulate(table, tablefmt='html'))\n",
    "\n",
    "    def clean_content(self, x):\n",
    "        return ' '.join(x.split()[1 : -1]) + ' ####' if  x!= '' and 'IRC:' in x.split()[0] else ' '.join(x.split()[0 : -1]) + ' ####'\n",
    "\n",
    "    def clean_documents(self, documents):\n",
    "        final_docs = []\n",
    "        for document in documents:\n",
    "            if document.metadata['page'] == 1:\n",
    "                index = documents.index(document)\n",
    "                while index < len(documents) and isinstance(documents[index].metadata['page'], int):\n",
    "                    final_docs.append(documents[index])\n",
    "                    index += 1\n",
    "        return final_docs\n",
    "\n",
    "    def load(self):\n",
    "        documents = []\n",
    "        for file in self.pdfs:\n",
    "            pdf = pdfplumber.open(file)\n",
    "            doc_name = str(file[len(self.directory_path): -4])\n",
    "            for page in pdf.pages:\n",
    "                doc_page = ''\n",
    "                tables = page.find_tables()\n",
    "                table_bboxes = [i.bbox for i in tables]\n",
    "                tables = [{'table': i.extract(), 'top': i.bbox[1]} for i in tables]\n",
    "                non_table_words = [word for word in page.extract_words() if not any([self.check_bboxes(word, table_bbox) for table_bbox in table_bboxes])]\n",
    "                for cluster in pdfplumber.utils.cluster_objects(non_table_words + tables, itemgetter('top'), tolerance=5):\n",
    "                    if 'text' in cluster[0]:\n",
    "                        try: \n",
    "                            doc_page += ' ' + ' '.join([i['text'] for i in cluster])\n",
    "                        except:\n",
    "                            pass                                # SOME PAGES ARE HORIZONTAL, FIX LATER\n",
    "                    elif 'table' in cluster[0]:\n",
    "                        doc_page += ' ' + self.format_table(cluster[0]['table'])\n",
    "                page_number = int(doc_page.split()[-1]) if doc_page != '' and doc_page.split()[-1].isdigit() else None\n",
    "                documents.append(Document(metadata={'source' : doc_name, 'page' : page_number}, page_content=self.clean_content(doc_page)))\n",
    "            \n",
    "        documents = self.clean_documents(documents)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Documents\n",
    "loader = pdf_loader(DIRECTORY_PATH)\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(separators=['\\n'], chunk_size=50, chunk_overlap=0, length_function=len)   \n",
    "parent_splitter = RecursiveCharacterTextSplitter(separators=['####'], chunk_size=2000, chunk_overlap=100, length_function=len)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings API integrated with LangChain\n",
    "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure parameters to create Matching Engine index\n",
    "ME_REGION = \"<region>\"\n",
    "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  # @param {type:\"string\"}\n",
    "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  # @param {type:\"string\"}\n",
    "ME_DIMENSIONS = 768  # when using Vertex AI PaLM Embedding\n",
    "\n",
    "#Make a Google Cloud Storage bucket for your Matching Engine index\n",
    "! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$ME_EMBEDDING_DIR\n",
    "\n",
    "#Create Index \n",
    "# NOTE : This operation can take upto 30 seconds\n",
    "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"langchain-index\",\n",
    "    dimensions=768,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\n",
    ")\n",
    "if my_index:\n",
    "    print(my_index.name)\n",
    "\n",
    "# Create an endpoint\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=f\"langchain-index-endpoint\", public_endpoint_enabled=True\n",
    ")\n",
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(\n",
    "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
    "    )\n",
    "\n",
    "# Deploy Index to endpoint\n",
    "# NOTE : This operation can take upto 20 minutes\n",
    "my_index_endpoint = index_endpoint.deploy_index(\n",
    "    index=my_index, deployed_index_id=\"langchain_index_endpoint_deployed_index\"\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Google's Matching Engine Vector Store\n",
    "me = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=my_index.name,\n",
    "    endpoint_id=index_endpoint.name,\n",
    "    stream_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text model instance integrated with LangChain\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@002\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kogo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
